{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5fe6e2-bc0f-4dbb-a2b9-08de318e1610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langgraph cassio dotenv langchain_community langchain_groq chromadb tiktoken langchain_huggingface sentence_transformers langchain_openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b109e64f-d60f-407c-9613-63f15de10503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade transformers deepspeed sentence-transformers langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f640d8-f5ff-4f58-837f-ff9c97f9b808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install deepspeed==0.9.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61d5776b-293e-4669-97fa-0bad69639ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cassio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "#provide connection of the AstraDB\n",
    "ASTRA_DB_APPLICATION_TOKEN=os.getenv('ASTRA_DB_TOKEN')\n",
    "ASTRA_DB_ID=\"53ed12c4-7e9d-4a83-8b47-9e44a9a27ed2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ed7273-f428-4719-a9a4-b3da6bf96954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ac7938-7ce4-4b5c-a43f-0cbcf52a17ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls=[\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load()for url in urls]\n",
    "docs_list=[item for sublist in docs for item in sublist]\n",
    "#text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500, chunk_overlap=0)\n",
    "doc_split=text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444aa4ec-335d-4507-99b6-507ae7bce39f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9012103f-2625-4f3e-a8e4-f06eaab7fe9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#doc_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c519757-b224-4333-bd68-90ff0888d6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c1b640a-43d5-4a65-924a-8bb2d43c89d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "astra_vectore_db=Cassandra(embedding=embeddings,\n",
    "                           table_name=\"qa_mini\",\n",
    "                           session=None, keyspace=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70747290-2654-40cc-b48f-c6187b99187e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 89\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "astra_vectore_db.add_documents(doc_split)\n",
    "print(\"Inserted\", len(doc_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16468586-2a93-45ab-a0cc-3c866da52eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "astra_vector_index=VectorStoreIndexWrapper(vectorstore=astra_vectore_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1801a24c-fdb2-4baa-b129-c3e46fd0e3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever=astra_vectore_db.as_retriever()\n",
    "#retriever.invoke(\"What is reward hacking?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "385c8f29-6a90-4809-b9e0-08dcc4e17a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating the router \n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "#router \n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\" Route a quser query to the most relevant data source \"\"\"\n",
    "    datasource: Literal[\"vectorstore\",\"wiki_search\"]=Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to wikipedia or the vectorstore\"\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0e21091-7ddb-4092-b6f1-726ab33c52bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "llm=ChatGroq(model_name=\"Gemma2-9b-It\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eabf7b95-04c9-4ccc-9a53-326345043738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structured_llm_router=llm.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e69ee17c-8b8c-4e89-9132-43a582a8bacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system=\"\"\" You are an expert at routing user questions between vectorstore or wikipedia tool\n",
    "Based on the questions asked please do the required routing\"\"\"\n",
    "route_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     (\"system\",system),\n",
    "        (\"human\",\"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a4303642-4a95-42c1-b1ae-1788ba71d6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_router=route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "accd1072-7479-408a-a93f-69f139a4a76c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip -q install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b7d19087-b73d-45cd-b612-854b5648d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='wiki_search'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Who is sharukh khan?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437752b3-8602-4dcc-a266-728c0ff8a391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95982d6a-83b7-4df9-a475-367aa1cf4f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc3cdece-edf5-4ff2-9f76-08b051ed9596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Page: Halo (franchise)\\nSummary: Halo is a military science fiction video game series and media franchise, originally developed by Bungie and currently managed and developed by Halo Studios (previously'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.run(\"Halo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5831510c-b0bf-4ded-86b9-c4af22678470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Langgraph application\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \n",
    "    question: str\n",
    "    generation:str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49d2fedc-e0b2-4f44-9649-87c415832f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def retrieve(state):\n",
    "    \"\"\"Retrieve documents\n",
    "    Args: Current graph state\n",
    "    \n",
    "    Returns state: new key added to state that contains retrieved docs\"\"\"\n",
    "    \n",
    "    print(\"---Retrieve---\")\n",
    "    questions=state[\"question\"]\n",
    "    #retrieval\n",
    "    \n",
    "    documents=retriever.invoke(questions)\n",
    "    return {\"documents\":documents,\"questions\":questions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a84713f8-4ccd-48d3-bd9c-fb9197804c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wiki_search(state):\n",
    "    \"\"\"\n",
    "    wiki search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---wikipedia---\")\n",
    "    print(\"---HELLO--\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "\n",
    "    # Wiki search\n",
    "    docs = wiki.invoke({\"query\": question})\n",
    "    #print(docs[\"summary\"])\n",
    "    wiki_results = docs\n",
    "    wiki_results = Document(page_content=wiki_results)\n",
    "\n",
    "    return {\"documents\": wiki_results, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "016294dd-5ae6-453c-9200-ee8f26129bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to wiki search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    if source.datasource == \"wiki_search\":\n",
    "        print(\"---ROUTE QUESTION TO Wiki SEARCH---\")\n",
    "        return \"wiki_search\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59242ed4-e056-4c3c-88b3-f60f2a0832f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f2ce69df990>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "workflow=StateGraph(GraphState)\n",
    "#define nodes\n",
    "workflow.add_node(\"wiki_search\",wiki_search)\n",
    "workflow.add_node(\"retrieve\",retrieve)\n",
    "\n",
    "#build the graph\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"wiki_search\":\"wiki_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\",END)\n",
    "workflow.add_edge(\"wiki_search\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c501be24-41d8-44be-b209-16449463afae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compile the graph\n",
    "graph=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ece29ece-0482-446b-b56e-fa0a1e789719",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAECCAIAAABizbXwAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3XdcU9ffB/CTnZCEsGTI3kMQBBTBiSiIGxXrLlJrVVxVilpbq1Zbf25rXUUrttZBrYWKWCuKC3CjggNk770yyXz+uD5IMSDUhHuTe94v/zDJzc2HJN+cc+44l6BQKAAEQe8goh0AgjAK1gYEKQdrA4KUg7UBQcrB2oAg5WBtQJByZLQDYJeIL6+vbOW3SAUtMqlUIZNowMZuGoNIoRGZuiSmHqWPORXtOJoN1kZH3CbZ68cthdn8VoGcziIxOSSmLpnFISvkGlAbCgBqSkT8FimVQSp5JbDzYNq5s2z66aCdSyMR4L6/NpJWRXpSXUu9xMCUaufOMrOjo53ogwh5soJsfmWhqKpQGDDRyM6DiXYiDQNr441nt5szkuoCJhp5DOWgnUXFGmsk6RfriARC8DwTEoWAdhyNAWsDAACu/lZtaEL1Hq2PdhA1qi0Tn99fOm25hbEVDe0smgHWBrj4U4WjN9vFl412kN4Qv6c0ZL4px4iCdhANgPfaiN9bOmCkvuMAFtpBek/83lL/cYaWznCA/h643r9x/WyNuz8HV4UBAJjxueXV36oFLTK0g2AdftuN5xktIr7MR6vHGJ1pFSn+jqucvLgv2kEwDb/tRmp8DT4LAwBAoxNMrGgPrzaiHQTTcFob6Rfr/ScYop0CTYPHGd77u14uRzsHhuGxNkQCeX1Fq08QThuNNoHhxo+vwaajU3isjcIsno5ubx8ss3bt2osXL/b0Wfn5+RMmTFBPImDhyHhxr1lNK9cCuKyNbL6te28fQPHy5ctee1Y36RpSyBRiQ5VYfS+h0XC3nUqhAPF7SmestiSo5+CJhISE06dPl5eX0+l0b2/v6OhoExMTX19f5FEWi3Xjxg2ZTBYbG/v333/X1NRwOJwRI0asXLmSwWAAAEaPHh0ZGXn37t0HDx7Mnj375MmTyBNXr149e/Zslad9dK2JTAaeI/RUvmZtoMCZ5jrxyW8L1bTyx48f+/j4XLhwobS0NCsra+HChREREQqForq62sfH5+zZs01NTQqF4pdffvHz87ty5UpxcXFGRsbYsWN37tyJrCEkJGTatGn79+9/+vQpl8vduXPnuHHjGhsbRSKROgK/uNuccrpaHWvWArg7Rp3fLGOqbbCRn59Po9EmTpxIJpMtLCy2b99eWVkJAOBwOAAAHR0d5D+hoaH+/v4ODg4AACsrq+Dg4LS0NGQNBAKBTqevWLECuUmj0QgEgp6eun7XmRwyv1mqppVrOvzVRotUfbXh6+tLIBAWLlw4efJkPz+/vn37Ghoq2VKsp6d36dKlrVu31tTUSKVSgUCgo/P2CI7+/furKd67dHTJ/BZYG8rhbiyuUBAoNHX91TY2NidOnLCwsDhw4MCkSZMiIiKys7PfXWznzp3Hjh2bMWNGbGzs6dOnw8LC2j/KYvXeMSxkMoFMwd13oJtw977osIkt9RL1rd/R0XHr1q1Xr149evQoiURatWqVWPyvDUEymSwxMfHjjz8eN26cubm5kZERj8dTX56u8ZqkFBo8o0M5/NWGOnsR2dnZz549AwCQSCQfH58lS5Y0NTXV19cjjyKbBOVyuUwmQwYeAAA+n3/r1i20thaqtYep6XBXG2w9CltfXWcvpKenr169+tq1a2VlZTk5OWfPnjUzMzM1NaXRaDQa7fHjxzk5OQQCwdnZOSkpqays7PXr16tWrRoyZEhLS0tRUZFU2rFo2Wx2XV1dZmYmMqZXObFIbtgXnuqkHO5qg0wFCoWiLFeojpVHRkaGhYXt27dv+vTpUVFRCoXihx9+IBAIAICIiIiUlJSlS5cKhcKNGzfKZLIZM2asX79+5syZUVFRpqam8+fPr6mp6bDCsWPHWlhYLFmyJDExUR2BXz3k9tXw0+LVB3f7/gAAT282tTRIh4UZoR0EZSK+7NT3xQu32qEdBKNw124AAGz7sbiNahyOa4rS1yK3wdo2cYQK4XEcpmtEptKJr+5zXQYpP0dcLBYHBwd39hCVqnxONFtb2xMnTqg06VtxcXFxcXFKH2KxWJ1t6fL19d21a1dn67yTUBu+ykJ1GbUNHvtUAAABV3ZmR8kn39oqfVShUHQ29uXxeDo6OkSikvaWQqH06dNH1Unf4HK5XC5X6UOtra00mvLxNI1GU7rzEZlzqLFGPGKaugJrAZzWBgDgwT8NTA7ZzU8X7SDoSDxSMT7SjEyFOzc6hcfxBmJgsMGr+y0V+WrZYIVxfxwoGxhsAAuja/itDQDA1OUWl36uFPLwdWLolV+rnbzZcNPte+G3T4WQy8GvW4vGRZr1scDFLrB/fq12Gahr5cJAO4gGwHW7AQAgEsHHG22una3Jf8pHO4t6ScWK+D2llk4MWBjdhPd2o82dhLrKIlHABENzBy386txNri95JRgZbmxsiYvmUSVgbbxVVSRKT6o3MKGa2tBt3Zk0hsY3qlVFovI84d3L9X5jDX1H6wM49u4JWBsdleYIcx62FGTzzR0YLD2yDpuko0vWYZNl0jdDdi6Xy2ZjZWLp+vr69nswiARCS4NEwJURCODFvRZdQ4qDJ8tzhJ6y/THQe8Da6FRFvujNNc24MgKBIBLIAAC1tbWVlZW9eWpe1/h8fk5Ojre3N3KTySERCQQdXRJbn2LhyGCwSGgH1GCwNnrmyJEjixcvRjvFvzx48IDP5w8dOpRMxuMRQOoDa6NbXrx4cevWLaxVRXtCofDQoUNr1qxBO4j2gP3Q9xOLxd9///2CBQvQDtIVBoNhZmZ26tQptINoD9hudEUqlWZnZ7u6unZ2MB/WIEPzW7duDR8+HO0sGg+2G51qbGwcMmSIjY2NphQGAADZZpWVlXX06FG0s2g82G4oV1JSwuVy+/Xrh3aQ/ygjI8Pf37+0tNTS0hLtLJoKthsdSSSS8PBwKpWquYUBAPD39wcAxMbGXrt2De0smgq2G//C4/GKi4sZDIadnZacRZ2QkDBlyhS0U2gk2G68tWnTJpFI1K9fP60pDAAAUhjr16/PzMxEO4uGgbXxxs8//+zj42NkpJ2Tj2zbtu3gwYMyGbw2bA/APhXIzMwcMGAApo6SUhOpVFpTU9O3L7w8bLfgvd04cuRIXl4eMoMg2lnUjkwm6+vrDx48uKWlBe0sGgC/tdHa2goAcHBwCA8PRztL72EwGLdv387NzUVxgmpNgdPaePTo0d69e5FriKGdpbdRKBRfX1+pVLphwwa0s2AaTmvjzJkz69atQzsFmvT09IYNG5acnIx2EOzC3Vg8NTU1MDAQ7RRYwePxWCzWw4cP267WCbXBUbuhUCgmTZpkb2+PdhAMQa4RFRcXd/fuXbSzYA5e2g0ej8fj8WQymbm5OdpZsCglJQWHQ6+u4aLduHjxYklJiampKSyMziCFsXnzZrSDYIj210Z5efmjR4/c3NzQDqIBPv7445UrV6KdAiu0vE9VUlJCpVJNTU3RDqIxRCIRnU4vKiqysbFBOwvKtLndiIiI4HA4sDB6hE6nAwDi4+PhsYla2248evSISqV6eHigHURTHTx4MCoqCu0UaNLC2hCJRK9evdKgk7yxLCkpacKECWinQEentdHU1IRc4FSzKBSKlJSUMWPGqGqFbRcCx6eLFy/KZDJ8nh3VaW3U1tZqYpMilUpVO4WZsbGxCtemiW7cuDFy5Ei0U6BAe8biCoVCJBLBuf1UbuTIkbm5uXfu3EE7SG/TktqQy+WNjY3INhZI5ZycnGpqarZt24Z2kF6lbX0qlYN9KtzShnZDIBCgHQEvMjMzHzx4gHaKXoJCbRQVFUVERKhqbfX19deuXduzZ4+qVgh1YcCAAampqefOnUM7SG9AYeSKnJ+tKoaGhqpdIdS1mJgYuVyuUCg0cRN/j3RrvPH48eOvvvpqz549Li4uyD2vXr1avXr11q1bvb298/Ly4uLi8vLyJBKJl5fXokWLTExMkMVSUlLOnz9fVVVlYmIybdq04ODgU6dOnT59Gnl00aJFU6ZMqa2tPXbsWGZmpkgkMjc3Dw8PHzVqFNK8LF26dOPGjXFxcXQ6fd++fdnZ2SdPniwqKpLJZHZ2dnPmzOnfv/+XX36ZlZWFrPDAgQP29vbPnz9H8gAAXFxcIiIinJ2dAQDfffcdgUCwsLC4cOHCunXr/Pz8mpqajh07lpWV1dLSYmNjExER4enp2eF9gOMNpS5cuBAYGKivr492EDUibdq0SekD7TvxxsbGycnJVCp1wIAByD2JiYkVFRVLly6tq6v7/PPPjY2Nv/jii8DAwLt37yYlJYWGhpJIpDt37uzatWvq1Knz5s3T1dU9dOiQlZXVmDFjRCJRQ0PD8ePHXV1dFQpFdHQ0j8dbs2ZNeHi4TCY7evSovb29hYUFn8+/ePFieXn55MmTx48fT6PRVq5c6enpuWLFipCQkMrKypMnT06YMGHkyJGZmZkeHh7bt283NjauqKiIjo52dnZevXp1SEjI8+fPT58+PWrUKCaTmZGRUVhYKJFIoqKi7O3tqVTq2rVrKyoqVq5cOXXq1IaGhmPHjvn5+XX4vJlMppo/Ao3k6OgYFBS0cOFCtIOoUbfGGyQSaejQoenp6W33pKWlDR8+nEQiJScnEwiEmJgYGxsbJyen6OjoqqqqtLQ0AMCff/7p7+8/ffp0R0fHsLCw6dOn19fX0+l0KpVKIBA4HA6NRnv48GFpaenq1as9PDzMzc3nzp3r5ub2119/AQCQJrt///7BwcE2Nja1tbUCgWDUqFFWVlbW1tZRUVGbN2+mUChMJpNEIlEoFA6HQyKRLl26xGAw1qxZY2tra2trGxMTI5PJ2uaEraysRF6Lw+FkZmbm5eWtWLHCy8vLysrqs88+MzY2Rl4aei8ymZyRkSGRSNAOokbdHYsPGzasoqKiqKgIGTBUVVUh+0pzcnKcnJyQUyuRFsbU1DQ/Px9ZzNHRsW0NkZGR7x56kJeXR6PR2s+x6eDgUFhY2HazrRdnbm5uYWGxc+fO+Pj458+fk0ik/v37v7tDIy8vz97evm0PIIPBMDc3LygoaFuJrq4u8v+cnBwKhdJ25T4ikdivX7+2JaH3IhAIL168ePbsGdpB1KW7Y3F3d3cDA4OMjAwbG5s7d+6YmJi4uroi12LMz8+fPHly25ISiaShoUEkEkkkkvfujOPz+XQ6vf2oTkdHp313rq1LQyKRduzYcf78+eTk5Li4OGNj43nz5gUFBXVYoUAgMDAwaH9P+xW27yAJBAKJRNK+XGUymXZ3oFXO09Nz7ty5X331VdtPmDbpbm0QicRhw4alp6fPmjUrPT19xIgRyP06Ojr9+vVbvnx5+4UZDAadTqfT6e/d88BkMoVCYfuNHkKhUEdHR+nCenp6kZGRCxYsKCsr+/PPP3fv3m1lZdW+aUJWyOfz29/D5/M7VEvbklQq9cCBAx3+zK4DQx0cP3788ePHaKdQix58FYYNG5afn//kyZOysrK22nBxcamoqDAzM7P8fwQCAfku2tnZZWdntz39yJEj715MyNHRUSwWt98I+/LlS2SzUgeVlZUZGRlEIpFEIllbWy9btoxIJBYXFyOPtm1Sc3R0RLaYITd5PF5ZWZmTk9O7K3RychKLxTKZrC05lUptf61uqDtoNBpyrQ/t04PacHV1NTY2jo2NtbGxsbW1Re4MDQ0VCoV79uzJz88vLy8/c+bMkiVLcnNzkdntHz9+/Ouvv+bm5iYmJiYlJSHfUSaT2dDQkJ2dXV1d7evra2Vl9cMPP+Tk5FRWVsbFxeXm5io9Irq2tnbbtm3x8fGlpaVlZWVnzpwhEolIU85isfLz8/Pz85ubmydMmNDa2rpv376ysrKioqIdO3Ywmcx3u14AAC8vL3t7+127dj179qyqqio1NXX58uWXLl36gDcTv8LDw+vq6tBOoWLd2oaLIBAIdXV1GRkZU6ZMabumEYvF8vb2zsjIOHPmzJUrVwQCQVRUFLKXwMrKSk9P7/Lly4mJieXl5fPmzQsODgYA9OnT58GDB4mJiXQ6fcCAAX5+frm5uadPn05ISBAKhVFRUQMHDgQAcLncv/76KygoyMzMDABgZGSkr6+fkpLy+++/X7lyRSgULl26FBnzsNns69evX7582d3d3cnJycPD486dO6dOnfrnn3/69OnzxRdfIKfFpqWl8fl8JAPSffL39y8sLDx37lxCQkJRUVFYWNjUqVM7/NVwG253WFtbX758efDgwWgHUSV4rOF7wH1/uKUZQ0+5XI5Mew5hVklJydWrV9FOoUqaURsdNj1BGGRlZXXo0KGSkhK0g6iMZvSpVH6ma/fBPlX31dXV8fl8a2trtIOohmacQQrPdNUIRkZG2nTBRA3oU3G5XLlcjnYKqFt++umnjIwMtFOoBtZrQyqVymQyuLtaU7i5uZ09exbtFKrR6XgDIz/VjY2NZDIZxQtVwrLsqZKSEgsLCy1437RwXkMIUglMF3djY2P7I3whjfDHH3/s3r0b7RQqgOnaePHixdChQ9FOAfWMj49PaWkp2ilUAPapIEg5TLcbEokElq4m0o4PDtO1MXHixPr6erRTQD32/fffa8GZ99itDZFIJBQKtWk/K35YWVlVVFSgneJDwfEGBCmH6XajoaEB7RTQfyEWi7XgNEDs1saNGze0YzM5DpWVlS1ZsgTtFB8Ku7UhEongzAYaSldXVyqVop3iQ8HxBgQph912A4LQhd3aOH78+KFDh9BOAf0XYrFYCyatwu75dGZmZmKxGO0U0H9BJBIDAgLQTvGh4HgDgpTDXLsxadIkZA5PuVxOIBCIRCLyfzjjIPZFRUUVFhZSKBSFQtHa2kqlUolEolQq1dDPDnO1YWlpee/evfb3KBQKPz8/9BJB3TV//vx169bV1NS0v1NzOyaYG4tHRka2XSIDweFw5s+fj14iqLv8/PyQWVjb09xBOeZqw8fHp/086gqFwsnJScsmWtVic+fO5XA4bTdZLNaCBQtQTfTfYa42AAALFixou2KGnp6e5r65OBQQEODg4NB208vLy8fHB9VE/x0Wa2PQoEHu7u5tjQYcbGiWiIgIpFdsYGDw8ccfox3nv8NibQAA5syZY2hoyOFwNPrNxSd/f3/kQiseHh5tFxbWRN3dTqWQg4ZqcXO9RC7rjc0OHLKzl+NYsVhsxHDPe8rrhVckkQi6hhR9E6qmzKvEb5bWV0rErTK0gygxKWiRoFZ33IiPe+ez6ykajWjYl6ajS+p6sW7t+3v9mPcsrVnEk/W1Z/BasPhhfDimLqmyQEhnkjwCdJ18UJsqrjuEPNm1szU1pSIrZ6ZQgIkp9jQLjU4szeGb2tDHzDGh0jv9LXx/beQ95T/PaB41sy8gdL2gVlCA6+cq3QayHb1ZaEdRTtAi+/Ng+fBppnomVLSzaLb68tb0izVTl5nTmcrL4z0diKIXgme3mkfNwkdhAAAIYNRMs+yMlsJsjF7x49fvikM/sYCF8eEMzWlBc/qe/l9xZwu8pzae3mzyn4S7C1AETDJ5erMJ7RRKPE5p8go0oNA0ZEiEeTpskssgvae3mpU+2tW7LBUrKgqFLD3MHVeibjq6pOrSVrEIc135ymIhS4+CdgqtwuSQq0tESh/qqjaa6yWm1gy1pcI0U2tGUz3mzuqUiRVsA9ibUiW2AaWzH8GuaoNAAAIu5r4fvUPAlRIA5g6SE/BkGLn2g9ZQyEErX/mmV9hzhSDlYG1AkHKwNiBIOVgbEKQcrA0IUg7WBgQpB2sDgpSDtQFBysHagCDlYG1AkHKwNiBIOYzWxjebYtZEa/zFTTRC21tdUJAXGOSblfWkwwKd3Y8FW7/7avnKT9S0cjSPP58ydfThQ7+YmfZ996EJE6ZKJRI0QuHOe99qoz7Gq1au69vXohdDYQJqtVFdXdXc3On5QwN94WRtveS9b7UuW3fypOm9FQdDVNyn2rR57eYt607EHQkdPzQj4zYAoKmp8bvtGz+aNX7suCFLl0VkPnkIAMh88nDm7AkAgNlzJn21cQ3Shpz/4/Ta9SuCx/rzeLz2fSqpVBp38uj8iGkhoQFz54cl/nUeAMDn80NCA06fiWt7aYlEMnHyyNhjP3b2ongjEAgCg3yfPctEbl67fiUwyBd59wAAJSVFgUG+L18976z7euq3n8dNGJaT+7Kbfarq6qrNW9aFTRsTEhrw8YLpF5MutD107fqVxUvmhY4fOnV68I8Hd4tEb84lkslkJ+KOzJ03JSQ0IPyj0H37twuFQuShDt8HAMCVK0kRkeHIyi///fbi5SQS6fad1HkfTx0TMjhy4Uevcl588Dv3hoprg0KhFBTm5b5+tf27H9zcPORy+dp1y58/f7Y2ZtPRw6dcnN3WrV9RUJDn4e618evvAQBHj5xav3YLAIBMJl9MumBn67B391E6nd5+nUeO7j8X/+ucWQuOHzsXPn3Ojwd3XUpOYDKZfoOG3L6T2rbYo0f3eDxe0Kixnb2oav9S7NPR0TE2Nsl+/hS5+ezZY2Njk6ysN6Xy9NljNovt7NRx+lrEjZspJ3/5aePX2ztb4F07dm6uq6/9btu+n4/HTw2buW//9gcP7wIA7ty5sXXbBh8fv9ifzsR88c2t29d2792GPOX8H6dPn4mLjFx6PPZszBffpKXfPPbzQeShDt+Hm7eu7di1ZWzIxB/2H58wPmzHzi03bqYgS9ZUV128+EdM9MY9u44QCITvt2/8sLftLRX3qRQAVFSU/bD/OEeXAwC4/yAj9/WrPbuPDPDyBQAsi4p++OjehT/PRq/5SkeHCQBgs3WZTCYAgEAg0Gn0zxat6LBCHo+X+Nfvc2YvCAmZAACwMLd8/frV6TNx48dNCQwM3vLt+tramj59jAEAN29ds7W1t7Nz6OJFVfvHYt8Ar4FZ2W9+7588fTR+XFjSpTc/50+fPfb2HkRUNhvXy5fZ2//3zeer1g/2G9L91yoozAub8pGrSz8AgPmk6U6OLiYmZgCA02fjPD29P124DPn4Pl24/Lvvv/70k2XGxiajg0IH+vrb2TkAACwsrAJHBt+7n4asrcP34ffzvw0dMnLmR/MBAM5Org0N9fV1tchDDY31hw/9wuHoAQCmhs3ctXsrj8djsVQwTYzqt1NZWlojhYG8yxQKxcvzzYyoRCKxv8eAvLwcpU/s16//u3fm5+dKpVJfn7d9Yk9Pn4qKMoFA4D94GJ1Ov5N2A+l3pWfcCho1tqcvqt18vAc9z36qUCgaGxvKy0snT5re3NxUWVUBAMjOfuLjo2Qy1arqyg1fr54RPndc6OQevVaA//AzZ+MOHd776PF9iUTi6upuYGAol8tzc1+2//iQz6Wg4DUAgMPRu3c/bemyiBkzx02dHnwx6Q8ut6Vtyfbfh9zcl87Obm03P1u0Ytq0Wcj/LS2skcIAAOjrGQAAhEJBj5J3RvVjcSbzbckKBHyJRBIS+vbyVjKZzMBA+ZWR2z+x/RoAAJ+v+YxAeDMLEDKhVkNjvYW5pf/gYbdvXw+bMiPzycOWluZRo0J6+qLazdt7EJfHLSoqKC4ptLdz5HD0nJ3dsp5lIsMDpbWx/4ftAoGgvr6up6/1+ar1drYOV1OSfz//G5PJnDRxeuSCJWKxWCaTxZ08+suvse0Xrm+oAwAc+HHn1ZTkz1eu7+fuSaPSzpw9eT31Stsybd8HkUgkkUjodOVTF9AZb+9HviSquuKHerdTMZksKpUae/R0+zuVtuNdrAEAsOHLrXa2Du3vN+5jAgAIDAzevGVdc0vz7dvX3dw8kM3BH/6iWsPQ0Mja2jb7+dP8/FwPjwEAAA93r6zsJwqFwryvRV8z83efMjoo1Nt70DebYvz9hw0dMrL7r0Umk6dNmzVt2qyGhvp/rl46/vMhPT396dNmk8nkqWEzx4+b0n5hPX0DmUyWfDlx3tyFY8aMQ+7k85VPEEqn0+l0OvIr2ZvU+41xcemH/HJYWdkg/6hUmpHR2wmv3lvidnaOFAqlsbGhbQ26uhwOR49KpQIABg0MoNFo9++np6XfRDpU3XlRXPHx8ct+/vTps8eent5IbTzLyszqpEMFAAgaNXb4sFFjQybu2r21+60Hj8e7mnJZKpUCAAwMDGd+NN/NzaOgII9IJDo6ulRXV7Z9FmZm5iQyWZetK5fLZTKZ7v93v/l8fnrGrc6+Dw4Ozs+ePW67eeDgrgMHd/X8zegZ9daGj/cgRwfn777/+smTR5VVFSnX/l702ezEv35HtpoDAO7evVNUVNDFGlgs1oQJU+NOHr2e+k9FZXnmk4fRMUu379iEPEqj0QICRpyL/6WpqTFw5Jj3vigOeXsNzMx8UFxc6OHuBQDo5+5ZVlby8NHdzmoDsSwqWoehs2Pn5m72TwgEwg8H/rdr99bXeTkVleUp1/7OzX3p5eUDAJj50fxbt6+fPhNXWlr8Oi/nu++/XrHyEz6fT6FQHB2cr/yTVF5Rlp//+suvVvn5DeFyW0pKipAaa2/6tNkPHt49EXfkVc6LPy6cTUiId3Vx/7A35v3U26cikUj/237g8NF932yOEYmEpqZ9581bGD59DgDAycl10KCAw0f2erh77dl9pIuVLF38OZvF/in2h/r6OgMDwwD/4Z9ERrU9Ompk8Jcplwf6DtbXN3jvi+KQp6dPQ0O9paW1np4+AIDNYtvY2BUW5nt5+XbxLCaTuX7dlpWff3rhz3MDulyybfn/bf/x2LEfV6/5TCwWm5r2XRCxeGzIRADA8GGjvlz/7ZmzcSfijjCZLHd3z727jyIbJ7+I3rhz15bIT2aYmvaNXLDE1cX9efbTJVHzj8We7bD+EcODVq1cF//7qTNnT5qYmK1YHjM6aOwHvzfv0dVc0Q1V4stxVZOWWKk7BAYlHS0dPce4jzkN7SD/cnZn6eBJxoam2Eql0WpKRE+u101bqeSIGDyOUCGoO3A31y30n02c3Olmq3Uxm4cMGdG7cdR5mNgvAAAOYUlEQVQO1gbUXT/9e7N4e8hONy0DawPqLqVnE2gxON6AIOVgbUCQcrA2IEg5WBsQpBysDQhSDtYGBCkHawOClIO1AUHKwdqAIOW6qg0ikcA2wOnVrNkGZDIZcz8cen0o2Lt4rcbjGCm/LHVXH7+eMaUiXyAV4+7TkEkVpTkCfRPM/S7QGMT6ila0U2iV2jIRnaW8Ct7z0+g6ULeyUDWzNmiQqkKhy0BdtFMoYdOP2VQNa0OVWurE1q5MpQ+9pzZGTO9zL7m2pQ5HU9PyGqXpf1UHzuiDdhAlbN2ZVDrh4T/1aAfREveSa3UNyZZOymcw6eq8P4RUovjt+2I3fwMmh6RvQlPItbOLRSQSGmtaeU3S5+mNc9ZbU6gEtBN16k5inZCvMDKnG1vQCZgbE2kAmUxRVyaqLhYZmJIHhXR6dP37awORmdpUnidUANBYJVZpzk6Jxa0KBaDReun8Tz1jCoEILBx0BgTq9c4rfoiCbEHBM26rUF5f0UsfRw8puFwem81GO4ZyBmZUOpPk6MWydtXpYrHu1kbvO378eGtr69KlS9EOAvWYWCweMWJERkYG2kE+CGySIUg5WBsQpBx2z4llsVjI5IWQJvLy8kI7wofCbm3weLzWVrgtX1M9eYLF6wP2CHZrg8lkkkgktFNA/4VCoXB2dkY7xYfC9HijtrYW7QjQf8Hn88vLy9FO8aGwWxsGBgYMhvIdlhDGiUSi/v2VXGlIs2C3NnR1dV++fIl2Cui/qKqqamlp6caCmIbd2jAxMem1neKQajU3N8PxhhrZ2treutXpxUogLHvx4oWBgcbPAord2gAAuLi45OTg8RqWmi4nJwe2G+oVEBCQn5+Pdgqox2QymZubWzcWxDRM14abm9v169fRTgH1zIsXL1paWvT19dEO8qEwXRtDhw5NS0tDOwXUM2lpaUOGDEE7hQpgujbIZPKYMWNu376NdhCoBx4+fDh69Gi0U6gApmsDADB+/Phz586hnQLqrszMTJlM5uDg0I1lsQ7rtTF48OCysrKysjK0g0Dd8vvvv4eHh6OdQjWwXhsAgE8++eTEiRNop4Der7y8PDc3NyQkBO0gqqEBtTFx4sSXL1/CHR3Yt3PnzlWrVqGdQmU0oDYAANHR0bt27UI7BdSVu3fvSiSSoUOHoh1EZbA7l0IH27dvd3Jymjp1KtpBIOWCg4Pj4+P19DRglpZu0pjaAACEhYXt37/fysoK7SBQR+vWrQsKChozZgzaQVRJM/pUiCNHjixevBjtFFBHf/zxh66urpYVhobVhomJyTfffLNixQq0g0BvZWZmpqSkfPnll2gHUT1Nqg0AgJ+fX1hYWHR0NNpBIAAAePXq1e7duw8fPox2ELXQsNoAAAQGBgYGBi5btgztIHh39+7dL7/88tSpU2gHURdNGou3V1RUtHv37gMHDqAdBKfS0tLOnDnz448/oh1EjTSv3UDY2NjMmjVLaw5P0Cznz58/d+6cdheGBrcbiIKCgpUrVx4+fNjCwgLtLHjxyy+/lJeXr1+/Hu0gaqfZtQEAEAgEs2bNWrRo0fjx49HOov0WLlwYEhKCk+Za42sDsXHjRgaDgYcfM7RkZWVFRkbGxsZqwUS33aQltYF0gjMzM5cvX25qaop2Fm1z8uTJ1NTUn3/+mUjU1AHqf6A9f+r06dMjIyMXLlwIz4VSocbGxoiICKFQGBcXh6vC0Kp2o82OHTtKSkr27t1LoWDuIsiaJTk5ec+ePfv27XN3d0c7Cwq0sDaQ3VKxsbGTJk2aPHky2lk0UnNz84YNG2xtbdesWYN2FtRoZ20gtmzZUllZuW3bNi2YY683nTt37ujRo9u2bfP390c7C5q0uTYAAPfv39+wYUNkZOSsWbPQzqIBCgsL4+LimExmTEwM2lnQp+W1gTh06FBKSsrXX389YMAAtLNg1969e9PS0jZt2oTP0cW7cFEbAIDi4uJvv/22X79+S5YsodPpaMfBluvXr2/atGnRokVz585FOwuG4GWrnLW19bFjx9zd3UePHv3rr7+iHQcr8vPzP/3004cPH16+fBkWRgd4aTfa27dvX2pq6tq1awMCAtDOghq5XH7s2LGUlJR169Z5e3ujHQeL8NJutLdq1aqDBw/Gx8cvX768pKSkw6OhoaEo5VKLkydPvjsD52+//ebn52dmZhYfHw8LozN4bDfapKennzhxwt7ePjo6mkwmAwDGjh3b0NAwZcoU7TjJs7i4OCoqqrKy8tGjR8g9N2/eTEhIsLS0XL16NdrpsA6P7UabgICA2NhYe3v7IUOGIIOQmpoauVyempqqHRNUf/vtt1VVVQQCYfTo0Xl5eYsXL05MTIyJiYGF0R24bjfa27dvX/vTO83NzRMSEggEAqqhPkhcXFxsbGxraysyunByclqzZs3AgQPRzqUxcN1utNdhssry8vLNmzejF+dD5efnx8fHI4UBACASieXl5bAwegTWxhvvXk7lxo0bqampKMX5UFu3bq2pqWl/j1AonDhxInqJNA8Z7QCYMH78eJFIBAAgEAhyuRzpSnG53B07dgQEBCCXcpa0ygU8GcBqD5RMJTLZJEAAAID9+/dnZ2cDANo6zAqFgkAgVFRUoJxSo8DxxhsJCQnNzc1isVgkErW0tAgEArmMAPjGAf1nNFSL+U0SAgHomerwG1vRTqocgUgQciVUOtHYiplXdq9emC0jN7FYLCaTyWAwaDQam82eMWMG2jE1CawNJfjN0ozkxtxHLRwTHaYhk6lPI1FIRJIGjMulYpm0Vd5SzeM3CvSMyC6+bNdBbLRDaSpYGx2lxtfmPeWZOhqxTXTQzvJBpK2yusIGsaB15LQ+Vi6a/begAtbGW811svMHyvTNdQ0sddHOojKtPAm3pqVPX9KIMEO0s2gYWBtvVJW0/nW4wt7fgkTRwm13dYWNFJJk8mdmaAfRJLA2AACgLL81Nb7O0kubJyhpKGmh02TjIozQDqIxtPA3sqd4TdLk4xXaXRgAAAMrXZGE/M+pmm4sCwFYGwAAcHZXmf1gXEwZamDBbqwDT242ox1EM+C9Nq7+VmNgxdHKMYZSJs6Gd5PrRQI52kE0AF6+E0pxG6RFLwXatFWqO0wcDW5dqEM7hQbAdW2kJdWbOOJuy6a+ObuqqLWpVop2EKzDb21IWuWF2TxdY+zuFNt5YNaFizvVsWYdQ2ZWWpM61qxN8FsbBdl8jgkT7RToYPfRyX/GRzsF1uG3NvKe8pkG2G001IrGpMjloLFagnYQTMPvMeqN1RJTV3VNVCWTSVNunniSdbWxqVKPYzI8YFbAoGkAgOqawp0HZi5ecOh2xtnCkqdEAtHTffSk0M9JJBIAoKD4yZ9Ju2pqCg30+4aOXqKmbAi2EaO6RKRvAqfT7hR+a6O5rtWcSlLTypOuHLj3MCFsYoytVf/c/PuJl/aQiGQ/38kkEhkAkHh577SJMQusdr7Of3A0bpmttZeXx2ihiBf32xdmpo4rl8TJZJJL/xzkctW4NUkBiPxmOBzvCk77VCK+jEwhqulscKGIl37v/IihcwcOGG9kaBkwaJrvgPHXb//StoBnv1E2Vv0BAI72Aw31zcvKXwIAXuamCYQtYROi+5o6Wpq7zZz6jUDYopZ8AAAASGQSD9ZGl3BaG0K+XN+UoaaVV1TmyuRSJ/tBbffY23rXN5S1tgqQm2amjm0P0elsoYiLdLcoFLqpsR1yvx7HmKNrrKaEAAAKg6xQaMAZKSjCaZ9Kh0VqqBQaO6tl5UgNHPl5KXjbMCkAAFxePXKDQqa1X14BFMizqJR/jX9oNDVuKhALJUR41lOXcFobNB2iXCpXyBUEoup/O+l0JgBgdvgWMxP79vdzOCbNzdWdPYtKoYtEvPb3CIVclWdrIxXLWHo4/fS7Cb/vjqE5QyaWk+mqH46bmTqSSBQer8HYPQi5h8dvBIBAIVO7eJZxH2uZXFpVU4B0qyqr89raGXUgEQCLAzdSdQW/tcExInMbhPp9WSpfM4PO8h8YdiU1lsnUszR3a2yqSry8V49j/MncPV08y8VpCI2qk5C0a1xwlEwmSb56mMVS4+Wmmqr5Zrb66lu/FsBvbTh6se5eaVZHbQAAJo5dyaCzL/3zYwu3js0ydHMeFjrmPfsrWEy9iNk7EpL3HDy2SF/PbNzopbcyzgL1zPkj4orpOkS2AX4//e7A73l/cjk4EpPvFmSDdhAU1BY0WdqAweNxd5xlj+B0Gy4AgEgEjt7sxnI1jncxq6Gs2XOEHtopsA7XrerwMKMTm4r0zTvdlvm//TOUDojlchmRQASd7Dtc//kFpg5HVSGPn1pdWPxU6UNMBocvVH4S34Y1iQy68u5ifXGz60BdBktdxwRoDfz2qRBpF+urygmG1sq/ylxeg0Ku5BQ5qUxCIpEJQHltsNmGKpyAnS9olkmVHxQolUrIZOXbmrrIkHOz+NPv7Ij47TF0F95rAwBw8ttiU1cTGhMXGzTLs6sHjdF19FLLFggtA389wLwN1nnpZWin6A11BY22rjRYGN0E2w2ATMOTcLTawsO0k16SNqjOa7R1Jg8KhkPw7oLtBgAAsPTIUz4zyb5aKOJq5+k+1Tl1fUwUsDB6BLYb/3J2dxmJRutjr8Yd0r2MVycUNPJcfXU8AvA1ncqHg7XR0f1/Gu9frjd1NtTvy9LoeasELa01rxtYbMKI6UZ9zGndeAb0L7A2lEtPasi600RnU5n6Ojr6dDKVRKZh/RIc0laZpFUmE8u4tbyWGoGFE9NzmK6Fo7pOU9F6sDa6UlkoynvKqykTc+slQp5U35TRWCVEO5RyRCKBSCYw2CRTa4a5Hc3WnUVnanCjhwWwNnpAIlZg9np/FBqm2zRNBGsDgpSDzS4EKQdrA4KUg7UBQcrB2oAg5WBtQJBysDYgSLn/Aw4dtvEiF52mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f2ce8c4a850>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91f08c66-7677-4ebb-9d81-0505832cc395",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---Retrieve---\n",
      "Assistant: {'documents': [Document(id='660830a175ff4abb8741a6f3bd35a19c', metadata={'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\"}, page_content=\"Generative-Model\\nVideo-Generation\\n\\n\\n\\nÂ« \\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n Â»\\n\\nThinking about High-Quality Human Data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(id='de98af38f7bd47df94e202e1acad4c52', metadata={'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\"}, page_content=\"Nlp\\nLanguage-Model\\nSafety\\nHallucination\\nFactuality\\n\\n\\n\\n« \\n\\nReward Hacking in Reinforcement Learning\\n\\n\\n »\\n\\nDiffusion Models for Video Generation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(id='2ae3c359036343c0adfdd41af6033e88', metadata={'description': 'Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/', 'title': \"Reward Hacking in Reinforcement Learning | Lil'Log\"}, page_content=\"Language-Model\\nRlhf\\nAlignment\\nSafety\\nReinforcement-Learning\\nLong-Read\\n\\n\\n\\n »\\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\"), Document(id='ff58062b2eaa456b831c546e7989e3a4', metadata={'description': 'Reward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to achieve high rewards, without genuinely learning or completing the intended task. Reward hacking exists because RL environments are often imperfect, and it is fundamentally challenging to accurately specify a reward function.\\nWith the rise of language models generalizing to a broad spectrum of tasks and RLHF becomes a de facto method for alignment training, reward hacking in RL training of language models has become a critical practical challenge. Instances where the model learns to modify unit tests to pass coding tasks, or where responses contain biases that mimic a user’s preference, are pretty concerning and are likely one of the major blockers for real-world deployment of more autonomous use cases of AI models.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-11-28-reward-hacking/', 'title': \"Reward Hacking in Reinforcement Learning | Lil'Log\"}, page_content='[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.')]}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: what is diffusion video\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---Retrieve---\n",
      "Assistant: {'documents': [Document(id='9bee944f879b48ac9bd44d205b0e8c9c', metadata={'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\"}, page_content=\"Diffusion Models for Video Generation | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Diffusion Models for Video Generation\\n    \\nDate: April 12, 2024  |  Estimated Reading Time: 20 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nVideo Generation Modeling from Scratch\\n\\nParameterization & Sampling Basics\\n\\nModel Architecture: 3D U-Net & DiT\\n\\n\\nAdapting Image Models to Generate Videos\\n\\nFine-tuning on Video Data\\n\\nTraining-Free Adaptation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\"), Document(id='522d2705bb0b49b0816438ed96a3e4e8', metadata={'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\"}, page_content='To achieve better scaling efforts, Sora (Brooks et al. 2024) leverages DiT (Diffusion Transformer) architecture that operates on spacetime patches of video and image latent codes. Visual input is represented as a sequence of spacetime patches which act as Transformer input tokens.\\n\\nFig. 5. Sora is a diffusion transformer model.(Image source: Brooks et al. 2024)\\nAdapting Image Models to Generate Videos#\\nAnother prominent approach for diffusion video modeling is to “inflate” a pre-trained image-to-text diffusion model by inserting temporal layers and then we can choose to only fine-tune new layers on video data, or avoid extra training at all. The prior knowledge of text-image pairs is inherited by the new model and thus it can help alleviate the requirement on text-video pair data.\\nFine-tuning on Video Data#\\nMake-A-Video (Singer et al. 2022) extends a pre-trained diffusion image model with a temporal dimension, consisting of three key components:\\n\\nA base text-to-image model trained on text-image pair data.\\nSpatiotemporal convolution and attention layers to extend the network to cover temporal dimension.\\nA frame interpolation network for high frame rate generation\\n\\n\\nFig. 6. The illustration of Make-A-Video pipeline.(Image source: Singer et al. 2022)\\nThe final video inference scheme can be formulated as:\\n\\n$$\\n\\\\hat{\\\\mathbf{y}}_t = \\\\text{SR}_h \\\\circ \\\\text{SR}^t_l \\\\circ \\\\uparrow_F \\\\circ D^t \\\\circ P \\\\circ (\\\\hat{\\\\mathbf{x}}, \\\\text{CLIP}_\\\\text{text}(\\\\mathbf{x}))\\n$$\\n\\nwhere:'), Document(id='698e214dc5614484984f104b0c7ade97', metadata={'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\"}, page_content='Fig. 1. Visualizing how the diffusion update step works in the angular coordinate, where DDIM evolves $\\\\mathbf{z}_{\\\\phi_s}$ by moving it along the $-\\\\hat{\\\\mathbf{v}}_{\\\\phi_t}$ direction. (Image source: Salimans & Ho, 2022)\\nThe $\\\\mathbf{v}$-parameterization for the model is to predict $\\\\mathbf{v}_\\\\phi = \\\\cos\\\\phi\\\\boldsymbol{\\\\epsilon} -\\\\sin\\\\phi\\\\mathbf{x} = \\\\alpha_t\\\\boldsymbol{\\\\epsilon} - \\\\sigma_t\\\\mathbf{x}$.\\nIn the case of video generation, we need the diffusion model to run multiple steps of upsampling for extending video length or increasing the frame rate. This requires the capability of sampling a second video $\\\\mathbf{x}^b$ conditioned on the first $\\\\mathbf{x}^a$, $\\\\mathbf{x}^b \\\\sim p_\\\\theta(\\\\mathbf{x}^b \\\\vert \\\\mathbf{x}^a)$, where $\\\\mathbf{x}^b$ might be an autoregressive extension of $\\\\mathbf{x}^a$ or be the missing frames in-between for a video $\\\\mathbf{x}^a$ at a low frame rate.\\nThe sampling of $\\\\mathbf{x}_b$ needs to condition on $\\\\mathbf{x}_a$ besides its own corresponding noisy variable. Video Diffusion Models (VDM; Ho & Salimans, et al. 2022) proposed the reconstruction guidance method using an adjusted denoising model such that the sampling of $\\\\mathbf{x}^b$ can be properly conditioned on $\\\\mathbf{x}^a$:'), Document(id='25693e7882cb4c9ab0d4d780b9986489', metadata={'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task—using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on “What are Diffusion Models?” for image generation before continue here.\\n', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\"}, page_content='Or\\n@article{weng2024video,\\n  title   = \"Diffusion Models Video Generation.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Apr\",\\n  url     = \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\\n}\\nReferences#\\n[1] Cicek et al. 2016. “3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation.”\\n[2] Ho & Salimans, et al. “Video Diffusion Models.” 2022 | webpage\\n[3] Bar-Tal et al. 2024 “Lumiere: A Space-Time Diffusion Model for Video Generation.”\\n[4] Brooks et al. “Video generation models as world simulators.” OpenAI Blog, 2024.\\n[5] Zhang et al. 2023 “ControlVideo: Training-free Controllable Text-to-Video Generation.”\\n[6] Khachatryan et al. 2023 “Text2Video-Zero: Text-to-image diffusion models are zero-shot video generators.”\\n[7] Ho, et al. 2022 “Imagen Video: High Definition Video Generation with Diffusion Models.”\\n[8] Singer et al. “Make-A-Video: Text-to-Video Generation without Text-Video Data.” 2022.\\n[9] Wu et al. “Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.” ICCV 2023.\\n[10] Blattmann et al. 2023 “Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.”\\n[11] Blattmann et al. 2023 “Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets.”\\n[12] Esser et al. 2023 “Structure and Content-Guided Video Synthesis with Diffusion Models.”')]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood Bye\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "while True:\n",
    "    user_input=input(\"User:\")\n",
    "    if user_input.lower() in [\"quit\",\"q\"]:\n",
    "        print(\"Good Bye\")\n",
    "        break\n",
    "    for event in graph.stream({'question': user_input}):\n",
    "        #print(event.values())\n",
    "        for value in event.values():\n",
    "               print(\"Assistant:\",value.get(\"answer\") or value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd790f-f1f2-4ec2-aa3d-c1c6ae97c78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70187eba-4f07-460b-b066-91f1b6c7a5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
